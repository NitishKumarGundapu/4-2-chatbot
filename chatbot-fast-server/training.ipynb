{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import random\n",
    "import torch as t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "words=[]\n",
    "classes = []\n",
    "documents = []\n",
    "ignore_words = ['?', '!']\n",
    "data_file = open('data.json').read()\n",
    "intents = json.loads(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for intent in intents['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        w = nltk.word_tokenize(pattern)\n",
    "        words.extend(w)\n",
    "        documents.append((w, intent['tag']))\n",
    "\n",
    "        if intent['tag'] not in classes:\n",
    "            classes.append(intent['tag'])\n",
    "\n",
    "words = [lemmatizer.lemmatize(w.lower()) for w in words if w not in ignore_words]\n",
    "words = sorted(list(set(words)))\n",
    "classes = sorted(list(set(classes)))\n",
    "\n",
    "\n",
    "pickle.dump(words,open('texts.pkl','wb'))\n",
    "pickle.dump(classes,open('labels.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = []\n",
    "output_empty = [0] * len(classes)\n",
    "for doc in documents:\n",
    "    bag = []\n",
    "    pattern_words = doc[0]\n",
    "    pattern_words = [lemmatizer.lemmatize(word.lower()) for word in pattern_words]\n",
    "    for w in words:\n",
    "        bag.append(1) if w in pattern_words else bag.append(0)\n",
    "    output_row = list(output_empty)\n",
    "    output_row[classes.index(doc[1])] = 1\n",
    "    \n",
    "    training.append([bag, output_row])\n",
    "random.shuffle(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x,train_y = [],[]\n",
    "\n",
    "for a in training:\n",
    "    train_x.append(a[0])\n",
    "    train_y.append(a[1])\n",
    "\n",
    "train_x = t.tensor(train_x)\n",
    "train_y = t.tensor(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "\n",
    "# class ChatbotRNN(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size, output_size):\n",
    "#         super(ChatbotRNN, self).__init__()\n",
    "\n",
    "#         self.input_size = input_size\n",
    "#         self.hidden_size = hidden_size\n",
    "#         self.output_size = output_size\n",
    "\n",
    "#         self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "#         self.rnn = nn.RNN(hidden_size, hidden_size)\n",
    "#         self.decoder = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "#     def forward(self, input, hidden):\n",
    "#         embeds = self.embedding(input)\n",
    "#         outputs, hidden = self.rnn(embeds, hidden)\n",
    "#         outputs = self.decoder(outputs)\n",
    "#         return outputs, hidden\n",
    "\n",
    "# def train(model, data , epochs , loss_fn):\n",
    "#     optimizer = optim.Adam(model.parameters())\n",
    "#     for epoch in range(epochs):\n",
    "#         for input, output in data:\n",
    "#             hidden = torch.zeros(1, model.hidden_size)\n",
    "#             output_pred, hidden = model(input, hidden)\n",
    "#             loss = loss_fn(output_pred, output)\n",
    "#             optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "# def evaluate(model, data):\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "#     for input, output in data:\n",
    "#         hidden = torch.zeros(1, model.hidden_size)\n",
    "#         output_pred, hidden = model(input, hidden)\n",
    "#         pred = output_pred.argmax(1)\n",
    "#         correct += (pred == output).sum()\n",
    "#         total += 1\n",
    "#     return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(t.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.l1 = t.nn.Linear(input_size, hidden_size)\n",
    "        self.l2 = t.nn.RNN(hidden_size, hidden_size)\n",
    "        self.l3 = t.nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.l1(x)\n",
    "        out,hid = self.l2(out)\n",
    "        out = self.l3(out)\n",
    "        out = t.nn.functional.softmax(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNet(len(train_x[0]), 16, len(train_y[0]))\n",
    "loss_fn = t.nn.CrossEntropyLoss()\n",
    "optimizer = t.optim.Adam(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NK\\AppData\\Local\\Temp\\ipykernel_1096\\4152925018.py:12: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  out = t.nn.functional.softmax(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<module 'torch' from 'c:\\\\Users\\\\NK\\\\.conda\\\\envs\\\\project\\\\lib\\\\site-packages\\\\torch\\\\__init__.py'> 0.7436683177947998\n",
      "<module 'torch' from 'c:\\\\Users\\\\NK\\\\.conda\\\\envs\\\\project\\\\lib\\\\site-packages\\\\torch\\\\__init__.py'> 1.743666172027588\n",
      "<module 'torch' from 'c:\\\\Users\\\\NK\\\\.conda\\\\envs\\\\project\\\\lib\\\\site-packages\\\\torch\\\\__init__.py'> 0.7436683177947998\n",
      "<module 'torch' from 'c:\\\\Users\\\\NK\\\\.conda\\\\envs\\\\project\\\\lib\\\\site-packages\\\\torch\\\\__init__.py'> 0.7436683177947998\n",
      "<module 'torch' from 'c:\\\\Users\\\\NK\\\\.conda\\\\envs\\\\project\\\\lib\\\\site-packages\\\\torch\\\\__init__.py'> 0.7436684370040894\n",
      "<module 'torch' from 'c:\\\\Users\\\\NK\\\\.conda\\\\envs\\\\project\\\\lib\\\\site-packages\\\\torch\\\\__init__.py'> 0.7436684370040894\n",
      "<module 'torch' from 'c:\\\\Users\\\\NK\\\\.conda\\\\envs\\\\project\\\\lib\\\\site-packages\\\\torch\\\\__init__.py'> 0.7442768812179565\n",
      "<module 'torch' from 'c:\\\\Users\\\\NK\\\\.conda\\\\envs\\\\project\\\\lib\\\\site-packages\\\\torch\\\\__init__.py'> 1.7436604499816895\n",
      "<module 'torch' from 'c:\\\\Users\\\\NK\\\\.conda\\\\envs\\\\project\\\\lib\\\\site-packages\\\\torch\\\\__init__.py'> 1.7435674667358398\n",
      "<module 'torch' from 'c:\\\\Users\\\\NK\\\\.conda\\\\envs\\\\project\\\\lib\\\\site-packages\\\\torch\\\\__init__.py'> 1.7436684370040894\n",
      "<module 'torch' from 'c:\\\\Users\\\\NK\\\\.conda\\\\envs\\\\project\\\\lib\\\\site-packages\\\\torch\\\\__init__.py'> 0.7436791658401489\n",
      "<module 'torch' from 'c:\\\\Users\\\\NK\\\\.conda\\\\envs\\\\project\\\\lib\\\\site-packages\\\\torch\\\\__init__.py'> 0.7436704635620117\n",
      "<module 'torch' from 'c:\\\\Users\\\\NK\\\\.conda\\\\envs\\\\project\\\\lib\\\\site-packages\\\\torch\\\\__init__.py'> 0.7436702251434326\n",
      "<module 'torch' from 'c:\\\\Users\\\\NK\\\\.conda\\\\envs\\\\project\\\\lib\\\\site-packages\\\\torch\\\\__init__.py'> 0.7436684370040894\n",
      "<module 'torch' from 'c:\\\\Users\\\\NK\\\\.conda\\\\envs\\\\project\\\\lib\\\\site-packages\\\\torch\\\\__init__.py'> 0.7436728477478027\n",
      "<module 'torch' from 'c:\\\\Users\\\\NK\\\\.conda\\\\envs\\\\project\\\\lib\\\\site-packages\\\\torch\\\\__init__.py'> 1.7436684370040894\n",
      "<module 'torch' from 'c:\\\\Users\\\\NK\\\\.conda\\\\envs\\\\project\\\\lib\\\\site-packages\\\\torch\\\\__init__.py'> 0.7436683177947998\n",
      "<module 'torch' from 'c:\\\\Users\\\\NK\\\\.conda\\\\envs\\\\project\\\\lib\\\\site-packages\\\\torch\\\\__init__.py'> 0.7436683177947998\n",
      "<module 'torch' from 'c:\\\\Users\\\\NK\\\\.conda\\\\envs\\\\project\\\\lib\\\\site-packages\\\\torch\\\\__init__.py'> 0.7436726093292236\n"
     ]
    }
   ],
   "source": [
    "for _ in range(100):\n",
    "    for a,b in zip(train_x,train_y):\n",
    "        y_pred = model(a.view(1, -1).float())\n",
    "        loss = loss_fn(y_pred, b.view(1,-1).float())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        if _ % 100 == 99:\n",
    "            print(t, loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.save(model.state_dict(), 'model/rnn_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
